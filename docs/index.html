<!DOCTYPE html>
<html lang="en">
    <head>
        <title> U-DREAM </title>
        <meta charset="utf-8">
        <link rel="stylesheet" href="style.css">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300&display=swap" rel="stylesheet"><meta name="viewport" content="width=device-width, initial-scale=1.0">
    </head>
    <body>
        <header>
            <div id="title_authors" class="titlecenter"> 
                <h1> U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model </h1>
            </div>
            <div class="center">
                <p style="text-align:center">
                <b>Louis Bahrman, Mathieu Fontaine, Gaël Richard</b>
                </p>
                <p style="text-align:center">LTCI, Télécom Paris, IP-Paris, France</p>
                <p style="text-align:center">Under review</p>
                <hr>
            </div>
            <div class="center" style="max-width:600px">
                <div class="container">
                    <a href="https://doi.org/10.48550/arXiv.2507.14237">Arxiv</a> &nbsp;
                    <a href="https://hal.science/hal-05158698">HAL</a> &nbsp;
                    <a href="https://www.github.com/Louis-Bahrman/UDREAM">Code</a> &nbsp;
                </div>
            </div>
        </header>
        <div class="center">
            <hr>
            <figure>
                <img class="figure" src="taslp_diagram.drawio.svg" alt="Block diagram">
            </figure>
            <section>
                <h2>Abstract</h2>
                <p>
                This paper explores the outcome of training state-of-the-art dereverberation models with supervision settings ranging from weakly-supervised to fully unsupervised,
                relying solely on reverberant signals and an acoustic model for training. 
                Most of the existing deep learning approaches typically require paired dry and reverberant data, which are difficult to obtain in practice.
                We develop instead a sequential learning strategy motivated by a bayesian formulation of the dereverberation problem, 
                wherein acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks, guided by a reverberation matching loss.
                Our most data-efficient variant requires only 100 reverberation-parameter-labelled samples to outperform an unsupervised baseline, 
                demonstrating the effectiveness and practicality of the proposed method in low-resource scenarios. 
                </p>    
            </section>
            <section>
                <h2>Audio examples</h2>
                <p>
                Examples are randomly drawn among several RT60 classes.
                Listening with headphones is recommended, as our proposed approaches mostly remove the late reverberation.
                </p>

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>RT60</th>
      <th>Reverberant input</th>
      <th>Ground truth</th>
      <th>Dry</th>
      <th>RIR</th>
      <th>Oracle parameters</th>
      <th>Acoustic analyzer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.29</th>
      <td><audio controls="">
    <source src="audios/0_reverberant_input.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/0_ground_truth.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/0_Dry.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/0_RIR.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/0_Oracle_parameters.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/0_Acoustic_analyzer.wav" type="audio/wav">
</audio></td>
    </tr>
    <tr>
      <th>0.49</th>
      <td><audio controls="">
    <source src="audios/1_reverberant_input.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/1_ground_truth.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/1_Dry.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/1_RIR.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/1_Oracle_parameters.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/1_Acoustic_analyzer.wav" type="audio/wav">
</audio></td>
    </tr>
    <tr>
      <th>0.73</th>
      <td><audio controls="">
    <source src="audios/2_reverberant_input.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/2_ground_truth.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/2_Dry.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/2_RIR.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/2_Oracle_parameters.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/2_Acoustic_analyzer.wav" type="audio/wav">
</audio></td>
    </tr>
    <tr>
      <th>1.07</th>
      <td><audio controls="">
    <source src="audios/3_reverberant_input.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/3_ground_truth.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/3_Dry.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/3_RIR.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/3_Oracle_parameters.wav" type="audio/wav">
</audio></td>
      <td><audio controls="">
    <source src="audios/3_Acoustic_analyzer.wav" type="audio/wav">
</audio></td>
    </tr>
  </tbody>
</table>
            </section>
            <section>
                <h2>Citing this work</h2>
                <p>
                If you use this work in your research or business, please cite it using the following BibTeX entry:
                </p>
                <pre>
        <code>
@article{bahrman2025udream,
      title={U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model}, 
      author={Louis Bahrman and Mathieu Fontaine and Gaël Richard},
      year={2025},
      eprint={2507.14237},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2507.14237}, 
}
        </code>
                </pre>
            </section>
        </div>
    </body>
</html>
