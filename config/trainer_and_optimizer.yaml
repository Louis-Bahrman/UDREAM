seed_everything: 12
trainer:
  max_epochs: 60
  # val_check_interval: 1.0
  # limit_val_batches: 100
  gradient_clip_val: 200.0
  logger:
    class_path: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: lightning_logs
      # log_graph: True
      default_hp_metric: False
      # class_path: lightning.pytorch.loggers.wandb.WandbLogger
  #     init_args:
  #         project: logs_dereverb_loss
  # profiler:
  #     class_path: lightning.pytorch.profilers.SimpleProfiler
  #     init_args:
  #         filename: profile
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelSummary
      init_args:
        max_depth: 2
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
  # - class_path: lightning.pytorch.callbacks.DeviceStatsMonitor
  #     init_args:
      #         cpu_stats: True
      # -
      #     class_path: lightning.pytorch.callbacks.BatchSizeFinder
      #     init_args:
      #         mode: power
      #         init_val: 4
      #         batch_arg_name: batch_size
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        every_n_epochs: 1
        # every_n_train_steps: 5000
        monitor: "val_dry_speech_model_ShortTimeObjectiveIntelligibility[0:-256]"
        mode: "max"
        save_top_k: -1 # 3
      # -
      #     class_path: lightning.pytorch.callbacks.EarlyStopping
      #     init_args:
      #         monitor: "val_dry_speech_model_ScaleInvariantSignalDistortionRatio[0:-256]"
      #         patience: 4
      #         mode: max
      #         verbose: true
  plugins:
      - class_path: lightning.pytorch.plugins.environments.SLURMEnvironment
        init_args:
          auto_requeue: False
optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 1e-4
# lr_scheduler:
#   class_path: ReduceLROnPlateau
#   init_args:
#     monitor: validation_step_rereverberation_loss_loss
